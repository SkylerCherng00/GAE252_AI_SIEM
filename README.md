# GAE252_AI_SIEM
## Testing
- Prepare the .ini files in MsgCenter/config folder
- Create virtual environment (or using poetry, uv ...etc)
  - `python -m venv gae252team2`
- Activate virtual environment
  - MAC: `source gae252team2/bin/activate`
- Install packages according to requirement.txt
  - `pip install -r requirement.txt`
- Set up configuration and vector database
  - Run `python startup.py` provided two features:
    - to generate `config.ini` from `config.ini.template`
    - to create tables in vector database from the `src` in Qrant folder
- Run `msg_api.py` in MsgCenter first
  - `python ./MsgCenter/msg_api.py`
- Then we can test the agent
  - `python ./AIAgent/agent.py`
- To test the analysis feature, open the webpage in `./AIAgent/agent_test.html`
  - The webpage will send a request to the agent API, which will return the analysis result
- Alternatively, we can browse the swagger UI at `http://localhost:8000/docs` to test the API endpoints directly after running the agent.

## About Debug Tools in the AIAgent Folder
- The debug tool is used to compare the analyzed report generated by the LLM with the answer report.
- The result is stored in the DebugComparisonReport collection in MongoDB.

### How to Use
- Change directory to the `GAE252_AI_SIEM` folder.
0. Prepare the logs for analysis and their corresponding answer reports.
  1. Under the `AIAgent/validation` folder, create a `logs` folder.
  2. Upload the logs to be analyzed into the `logs` folder.
  3. Upload the answer reports into the `reports` folder.
  4. **ATTENTION**: The name of each answer report MUST follow the format: `THE_LOG_NAME_report.md`. 
    - For example, for `DDoS.log`, the corresponding answer report should be named `DDoS_report.md`.
1. Activate `msg_api.py` in the `MsgCenter` folder.
2. Activate `debug_agent.py` in the `AIAgent` folder.
3. Execute `debug_tool.py`.
4. Open a MongoDB management tool such as MongoDB Compass and locate the `DebugComparisonReport` collection.
  - **input_report** is the report produced by the LLM.
  - **comparison_result** is the analysis comparing the LLM report and the answer report.
  - **analyzing_time** is the time taken by the LLM to process the logs (unit: seconds).
  - **answer_report** is the filename of the answer report.
  - **use_model** is the LLM model used for analyzing the logs.
