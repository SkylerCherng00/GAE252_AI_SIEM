from fastapi import FastAPI, HTTPException, Body
from pydantic import BaseModel, Field
from typing import Any, Optional
from llm_factory import LLMExecutorFactory, LLMExecutor
import uvicorn

# Initialize FastAPI application with metadata
app = FastAPI(title="AI SIEM Log Analysis API", 
              description="API for analyzing logs using different LLM models", 
              version="1.0.0")

# Initialize the LLM Factory singleton to manage model creation
llm_factory = LLMExecutorFactory()

# Cache for the current executor to avoid recreating model instances
CURRENT_EXECUTOR = None
CURRENT_EXECUTOR_TYPE = "ollama"  # Default to ollama as requested

class LogAnalysisRequest(BaseModel):
    """
    Request model for log analysis
    
    Attributes:
        logs: The raw log data to be analyzed by the LLM
        model_type: Optional type of model to use (e.g., 'ollama', 'gemini', 'azure')
        model_name: Optional specific model name if the executor supports multiple models
    """
    logs: str = Field(..., description="The logs to analyze")
    model_type: Optional[str] = Field(None, description="The model type to use for analysis")
    model_name: Optional[str] = Field(None, description="The specific model name to use (if supported by the executor)")
    
class ModelInfo(BaseModel):
    """
    Information about an available model type
    
    Attributes:
        name: The identifier of the model (e.g., 'ollama', 'gemini', 'azure')
        description: Human-readable description of the model
        is_current: Whether this model is currently active
    """
    name: str
    description: str
    is_current: bool

class APIResponse(BaseModel):
    """
    Standard API response model for consistent return formats
    
    Attributes:
        success: Whether the operation was successful
        message: A descriptive message about the result
        data: Optional data payload returned by the operation
    """
    success: bool
    message: str
    data: Optional[Any] = None

def _get_executor(model_type: Optional[str] = None) -> LLMExecutor:
    """
    Get the appropriate LLM executor based on the requested model type
    
    This function implements a singleton-like pattern by caching the current executor
    and only creating a new one when necessary. It also handles fallback logic when
    a requested model is not available.
    
    Args:
        model_type: The type of model to use ('ollama', 'gemini', 'azure', or None for default)
        
    Returns:
        An instance of LLMExecutor ready to process requests
        
    Raises:
        HTTPException: If the requested model is not available or cannot be initialized
    """
    global CURRENT_EXECUTOR, CURRENT_EXECUTOR_TYPE
    
    # If no model specified, use the current one
    if model_type is None:
        model_type = CURRENT_EXECUTOR_TYPE
    
    # If requesting the current model and we have it cached, return it
    if model_type == CURRENT_EXECUTOR_TYPE and CURRENT_EXECUTOR is not None:
        return CURRENT_EXECUTOR
    
    # Otherwise, try to create the requested executor
    executor = llm_factory.create_executor(model_type)
    
    if executor is None:
        available = llm_factory.get_available_executors()
        available_str = ", ".join(available) if available else "None"
        raise HTTPException(
            status_code=400,
            detail=f"Requested model '{model_type}' is not available. Available models: {available_str}"
        )
    
    # Update the cache
    CURRENT_EXECUTOR = executor
    CURRENT_EXECUTOR_TYPE = model_type
    
    return executor

@app.post("/switch-model", response_model=APIResponse)
async def switch_model(model_type: str = Body(..., embed=True)):
    """
    Switch the active LLM model to a different type
    
    This endpoint allows the client to change which LLM model/service is used for analysis.
    It validates that the requested model is available before switching and provides
    appropriate error messages if the model cannot be used.
    
    Args:
        model_type: The type of model to switch to ('ollama', 'gemini', 'azure')
        
    Returns:
        APIResponse: A standardized response indicating success or failure
    """
    global CURRENT_EXECUTOR
    try:
        # Try to get the executor for the requested model
        CURRENT_EXECUTOR = _get_executor(model_type)
        
        return APIResponse(
            success=True,
            message=f"Successfully switched to model: {model_type}",
            data={"model_type": model_type}
        )
    except HTTPException as e:
        return APIResponse(
            success=False,
            message=e.detail,
            data=None
        )
    except Exception as e:
        return APIResponse(
            success=False,
            message=f"Error switching model: {str(e)}",
            data=None
        )

@app.post("/analyze-logs", response_model=APIResponse)
async def analyze_logs(request: LogAnalysisRequest):
    """
    Analyze logs using the specified LLM model
    
    This is the main endpoint for log analysis. It takes log data from the client,
    formats it into a prompt for the LLM, and returns the analysis results. The client
    can optionally specify which model to use for the analysis.
    
    Args:
        request: The LogAnalysisRequest object containing logs and optional model preferences
        
    Returns:
        APIResponse: A standardized response with the analysis results or error details
    """
    try:
        # Get the executor for the requested model type (if specified)
        executor = _get_executor(request.model_type)
        
        # Prepare the prompt for log analysis
        prompt = f"""
        Analyze the following logs and identify any security issues, anomalies, or concerns.
        Provide a detailed explanation of your findings and recommendations for action.
        
        LOGS:
        {request.logs}
        """
                    
        analysis = executor.generate_response(prompt)
        
        return APIResponse(
            success=True,
            message="Log analysis completed successfully",
            data={"analysis": analysis, "model_used": CURRENT_EXECUTOR_TYPE}
        )
    except HTTPException as e:
        return APIResponse(
            success=False,
            message=e.detail,
            data=None
        )
    except Exception as e:
        return APIResponse(
            success=False,
            message=f"Error analyzing logs: {str(e)}",
            data=None
        )

@app.get("/health")
async def health_check():
    """
    Health check endpoint for monitoring and status verification
    
    This endpoint provides information about the API's health status, including
    which LLM models are currently available and which one is active. It's useful
    for monitoring systems and service discovery.
    
    Returns:
        dict: A dictionary with the health status and relevant information
    """
    try:
        available_models = llm_factory.get_available_executors()
        return {
            "status": "healthy",
            "available_models": available_models,
            "current_model": CURRENT_EXECUTOR_TYPE
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }

@app.get("/models", response_model=APIResponse)
async def list_models():
    """
    List all available LLM models/executors
    
    This endpoint returns information about all LLM models that are currently
    available for use, including which one is active. This helps clients
    understand their options for model switching.
    
    Returns:
        APIResponse: A standardized response with the list of available models
    """
    try:
        available = llm_factory.get_available_executors()
        
        models_info = []
        for model in available:
            models_info.append(ModelInfo(
                name=model,
                description=f"LLM executor for {model.capitalize()}",
                is_current=(model == CURRENT_EXECUTOR_TYPE)
            ))
        
        return APIResponse(
            success=True,
            message="Available models retrieved successfully",
            data=models_info
        )
    except Exception as e:
        return APIResponse(
            success=False,
            message=f"Error retrieving models: {str(e)}",
            data=None
        )

if __name__ == "__main__":
    uvicorn.run("agent:app", host="0.0.0.0", port=8000, reload=True)

# ============================================================================
# Curl Testing Script
# ============================================================================
# This section contains curl commands for testing the API endpoints
# You can copy these commands and run them in a terminal to test the API
#
# 1. Health Check - Verify API is running and check available models
# curl -X GET http://localhost:8000/health
#
# 2. List Models - Get all available LLM models
# curl -X GET http://localhost:8000/models
#
# 3. Switch Model - Change to a different LLM model (example: switch to gemini)
# curl -X POST http://localhost:8000/switch-model \
#     -H "Content-Type: application/json" \
#     -d '{"model_type": "gemini"}'
#
# 4. Analyze Logs - Send logs for analysis with default model
# curl -X POST http://localhost:8000/analyze-logs \
#     -H "Content-Type: application/json" \
#     -d '{
#         "logs": "2024-07-31T10:15:22.123Z ERROR [auth-service] Failed login attempt for user admin from IP 192.168.1.100 - Invalid password (attempt 5 of 5)"
#     }'
